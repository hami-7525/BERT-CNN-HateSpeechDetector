# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oHrdqrn04hpR_kQ8dMph8j2XALiV970g
"""

!pip3 install transformers
!pip3 install nltk
import numpy as np
import torch.nn as nn
import torch
import torch.nn.functional as F
import transformers
import matplotlib.pyplot as plt
import pandas as pd
from torch.utils.data import TensorDataset,DataLoader
from torch.autograd import Variable
from sklearn.metrics import accuracy_score
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

import pandas as pd
import numpy as np
import torch
import re
import tqdm
from matplotlib._path import (affine_transform, count_bboxes_overlapping_bbox,
     update_path_extents)

dataset=pd.read_csv('labeled_data (1).csv')
dataset.dropna(inplace = True)
df = dataset
dataset



print("number of tweets belonging to classes 0,1 and 2")
dataset.groupby('class')['id'].nunique()



dataset.groupby('class')['id'].nunique().plot(kind='bar',title='Plot of number of tweets belonging to a particular class')

X = df.drop(['class'], axis=1)
y = df['class']

y.value_counts()

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(sampling_strategy='auto') # Use 'auto' to balance all classes
X_res, y_res = rus.fit_resample(X, y)

# Plot the distribution of the classes after under-sampling
ax = y_res.value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Under-sampling")

from nltk import tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from gensim.models import Word2Vec

from sklearn.model_selection import train_test_split

import nltk
nltk.download('stopwords')

stop_words= set(stopwords.words('english'))

import nltk
nltk.download('punkt')

def clean_tweet(tweet):
    tweet = re.sub("#", "",tweet) # Removing '#' from hashtags
    tweet = re.sub("[^a-zA-Z#]", " ",tweet) # Removing punctuation and special characters
    tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+',"<URL>", tweet)
    tweet = re.sub('http','',tweet)
    tweet = re.sub(" +", " ", tweet)
    tweet = tweet.lower()
    tweet = word_tokenize(tweet)
    return_tweet=[]
    for word in tweet:
        if word not in stop_words:
            return_tweet.append(word)
    return return_tweet
dataset["tweet"]=dataset["tweet"].apply(clean_tweet)

model = Word2Vec(dataset["tweet"].values, window=5, min_count=1, workers=4)

def get_features(tweet):
    features=[]
    for word in tweet:
        features.append(model.wv[word])
    return np.mean(features,0)

dataset["features"]=dataset["tweet"].apply(get_features)

data=[]
for i in dataset["features"].values:
    temp=[]
    for j in i:
        temp.append(j)
    data.append(temp)
data=np.array(data)

from sklearn.preprocessing import label_binarize

Y = label_binarize(dataset["class"].values, classes=[0, 1, 2])
n_classes = Y.shape[1]
X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=42)

print(X_train)
print(y_train)

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn import svm
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

svm_clf = OneVsRestClassifier(svm.SVC(gamma='scale', probability=True))
svm_clf.fit(X_train,y_train)
y_pred = svm_clf.predict(X_test)
f = f1_score(y_test, y_pred, average='micro')
print("F1 Score: ", f)
p = precision_score(y_test, y_pred, average='micro')
print("Precision Score: ", p)
r = recall_score(y_test, y_pred, average='micro')
print("Recall Score: ", r)
print("Accuracy: ", svm_clf.score(X_test,y_test))

y_score = svm_clf.predict_proba(X_test)
precision = dict()
recall = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],
                                                        y_score[:, i])
    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc = "center right")
plt.title("Precision vs. Recall curve")
plt.show()

model_class = transformers.BertModel
tokenizer_class = transformers.BertTokenizer
pretrained_weights = 'bert-base-uncased'

tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
bert_model = model_class.from_pretrained(pretrained_weights)

max_seq = 27
def tokenize_text(df, max_seq):
    return [
        tokenizer.encode(text, add_special_tokens=True)[:max_seq] for text in df["tweet"].values
    ]
def pad_text(tokenized_text, max_seq):
    return np.array([el + [0] * (max_seq - len(el)) for el in tokenized_text])
def tokenize_and_pad_text(df, max_seq):
    tokenized_text = tokenize_text(df, max_seq)
    padded_text = pad_text(tokenized_text, max_seq)
    return torch.tensor(padded_text)
def targets_to_tensor(df, target_columns):
    return torch.tensor(df[target_columns].values, dtype=torch.float32)

max = 0
for i in dataset["tweet"]:
  if len(i) > max:
    max = len(i)
print(max)

for i in range(0,len(dataset["tweet"])):
  dataset["tweet"][i] = " ".join(dataset["tweet"][i])

dataset["tweet"]

indices = tokenize_and_pad_text(dataset, max_seq)

[no,_] = list(indices.shape)

stepSize = 32
X = np.empty([0,max_seq,768])

for i in range(0,no,stepSize):
  if no - i <= stepSize:
    n = no
  else:
    n = i+stepSize
  with torch.no_grad():
    x = bert_model(indices[i:n,:])[0].detach().numpy()
  X = np.vstack([X,x])

label_col = ["class"]
Y = targets_to_tensor(dataset,label_col)

x_train,x_val,y_train,y_val = train_test_split(X,Y,test_size=0.2,random_state=42)
x_train = torch.tensor(x_train,dtype=torch.float32)
x_val = torch.tensor(x_val,dtype=torch.float32)

loss_func = nn.CrossEntropyLoss()

class HTSPC_CNN(nn.Module):
    def __init__(self,embed_num,embed_dim,class_num,kernel_num,kernel_sizes,dropout,static):
        super(HTSPC_CNN, self).__init__()

        V = embed_num
        D = embed_dim
        C = class_num
        Co = kernel_num
        Ks = kernel_sizes

        self.static = static
        self.emed_layer = nn.Embedding(V, D)

        self.conv1 = nn.ModuleList([nn.Conv2d(1,Co,(K, D)) for K in Ks])
        self.dropout = nn.Dropout(dropout)

        self.lin1 = nn.Linear(len(Ks)* Co, 3)
        self.sigmoid = nn.Sigmoid()


    def forward(self,xb):
        if self.static:
            x = Variable(xb)
        x = x.unsqueeze(1)

        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv1]
        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]

        x = torch.cat(x, 1)
        x = self.dropout(x)
        x = self.lin1(x)
        return x

def loss_batch(model, loss_func, xb, yb, opt=None):
    yb = yb.squeeze_()
    # print(model(xb).squeeze_())
    # print(yb)
    loss = loss_func(model(xb.float()), yb.long())

    if opt is not None:
        loss.backward()
        opt.step()
        opt.zero_grad()

    return loss.item(), len(xb)

def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    for epoch in range(epochs):
        model.train()
        for xb, yb in train_dl:
            loss_batch(model, loss_func, xb, yb, opt)

        model.eval()
        with torch.no_grad():
            losses, nums = zip(
                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]
            )
        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)

        print(epoch, val_loss)

def get_data(train_ds, valid_ds, bs):
    return (
        DataLoader(train_ds, batch_size=bs, shuffle=True),
        DataLoader(valid_ds, batch_size=bs * 2),
    )

bs = 128
epochs = 50
learning_rate = 0.0001

train_dataset = TensorDataset(x_train, y_train)
valid_dataset = TensorDataset(x_val  , y_val  )

embed_num = x_train.shape[1]
embed_dim = x_train.shape[2]
class_num = y_train.shape[1]

print(class_num)

kernel_num = 3
kernel_sizes = [2,3,4]
dropout = 0.5
static = True

train_dl, valid_dl = get_data(train_dataset, valid_dataset, bs)
model = HTSPC_CNN(embed_num,embed_dim,class_num,kernel_num,kernel_sizes,dropout,static)

opt = torch.optim.Adam(model.parameters(), lr = learning_rate)
fit(epochs, model, loss_func, opt, train_dl, valid_dl)

_,preds = torch.max(model(x_val),1)

print(accuracy_score(preds.detach().numpy(),y_val.detach().numpy()))

f = f1_score(preds.detach().numpy(),y_val.detach().numpy(), average='weighted')
print("F1 Score: ", f)
p = precision_score(preds.detach().numpy(),y_val.detach().numpy(), average='weighted')
print("Precision Score: ", p)
r = recall_score(preds.detach().numpy(),y_val.detach().numpy(), average='micro')
print("Recall Score: ", r)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming you have your predictions and true labels stored in preds and y_val respectively

# Convert predictions and true labels to numpy arrays if they are tensors
preds = preds.detach().numpy()
y_val = y_val.detach().numpy()

# Compute confusion matrix
conf_matrix = confusion_matrix(y_val, preds)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

import joblib

# Save the trained model
joblib.dump(model, 'trained_model1.pkl')



from sklearn.metrics import classification_report
print(classification_report(y_val, preds))

import matplotlib.pyplot as plt

# Create empty lists to store training and validation accuracy
train_acc_history = []
val_acc_history = []

def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    train_loss_history = []
    val_loss_history = []
    val_acc_history = []

    for epoch in range(epochs):
        model.train()
        epoch_losses = []
        for xb, yb in train_dl:
            loss, _ = loss_batch(model, loss_func, xb, yb, opt)
            epoch_losses.append(loss)

        train_loss = np.mean(epoch_losses)
        train_loss_history.append(train_loss)

        model.eval()
        with torch.no_grad():
            # Calculate validation loss
            val_losses = [loss_batch(model, loss_func, xb, yb)[0] for xb, yb in valid_dl]
            val_loss = np.mean(val_losses)
            val_loss_history.append(val_loss)

            # Convert validation predictions and labels to NumPy arrays
            val_preds = torch.argmax(model(x_val), axis=1).detach().numpy()
            y_val_np = y_val

            # Calculate validation accuracy
            val_acc = accuracy_score(val_preds, y_val_np)
            val_acc_history.append(val_acc)

        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    return train_loss_history, val_loss_history, val_acc_history

# Call the fit function
train_loss_history, val_loss_history, val_acc_history = fit(epochs, model, loss_func, opt, train_dl, valid_dl)

